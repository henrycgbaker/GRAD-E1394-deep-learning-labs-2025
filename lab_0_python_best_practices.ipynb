{
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 0: Python Best Practices for Deep Learning\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "1. Write type-annotated Python functions using modern syntax\n",
    "2. Apply Pythonic patterns (comprehensions, context managers, enumerate/zip)\n",
    "3. Perform NumPy array operations including broadcasting and reshaping\n",
    "4. Understand OOP conventions used in PyTorch (classes, `__call__`, etc.)\n",
    "5. Write generators for memory-efficient data processing\n",
    "6. Debug Python code effectively\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic Python programming (variables, loops, functions, classes)\n",
    "- Familiarity with importing packages\n",
    "\n",
    "## Why This Lab?\n",
    "\n",
    "Deep learning code has conventions that may be unfamiliar:\n",
    "- **Type hints** make code self-documenting and catch bugs early\n",
    "- **NumPy broadcasting** is essential for understanding tensor operations\n",
    "- **Generators** power PyTorch DataLoaders\n",
    "- **OOP patterns** like `__call__` are central to `nn.Module`\n",
    "\n",
    "This lab ensures you have the Python foundations needed for Labs 6-10."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ==== Environment Setup ====\n",
    "import os\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running on Google Colab\")\n",
    "else:\n",
    "    print(\"Running locally\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ==== Device Setup ====\n",
    "import torch\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"Get best available device: CUDA > MPS > CPU.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(f\"Using CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "        print(\"Using Apple MPS (Metal)\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "DEVICE = get_device()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Python Foundations for Deep Learning\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 Type Hints\n",
    "\n",
    "Type hints make code self-documenting and enable better IDE support.\n",
    "\n",
    "### Basic Syntax (Python 3.10+)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Basic type hints\n",
    "def greet(name: str) -> str:\n",
    "    return f\"Hello, {name}!\"\n",
    "\n",
    "# Collections - use lowercase (Python 3.10+)\n",
    "def average(values: list[float]) -> float:\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "# Dictionaries\n",
    "def word_count(text: str) -> dict[str, int]:\n",
    "    words = text.lower().split()\n",
    "    return {word: words.count(word) for word in set(words)}\n",
    "\n",
    "# Optional values (can be None)\n",
    "def find_index(items: list[str], target: str) -> int | None:\n",
    "    try:\n",
    "        return items.index(target)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# Test\n",
    "print(f\"greet('World'): {greet('World')}\")\n",
    "print(f\"average([1, 2, 3, 4, 5]): {average([1, 2, 3, 4, 5])}\")\n",
    "print(f\"word_count('the cat and the dog'): {word_count('the cat and the dog')}\")\n",
    "print(f\"find_index(['a', 'b', 'c'], 'b'): {find_index(['a', 'b', 'c'], 'b')}\")\n",
    "print(f\"find_index(['a', 'b', 'c'], 'x'): {find_index(['a', 'b', 'c'], 'x')}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Q: Why use `int | None` instead of `Optional[int]`?</b></summary>\n",
    "\n",
    "**A:** `int | None` is the modern Python 3.10+ syntax. It's more readable and doesn't require importing from `typing`. The older `Optional[int]` still works but is more verbose.\n",
    "\n",
    "```python\n",
    "# Old style (pre-3.10)\n",
    "from typing import Optional, List\n",
    "def f(x: Optional[int]) -> List[str]: ...\n",
    "\n",
    "# Modern style (3.10+)\n",
    "def f(x: int | None) -> list[str]: ...\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Exercise: Add Type Hints\n\nAdd type hints to the following functions:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Exercise: Add type hints to these functions\n\ndef calculate_loss(predictions, targets):\n    \"\"\"Calculate mean squared error loss.\"\"\"\n    return sum((p - t) ** 2 for p, t in zip(predictions, targets)) / len(predictions)\n\ndef get_batch(data, batch_idx, batch_size):\n    \"\"\"Get a batch from data. Returns None if batch_idx out of range.\"\"\"\n    start = batch_idx * batch_size\n    if start >= len(data):\n        return None\n    return data[start:start + batch_size]\n\ndef create_optimizer_config(lr, momentum, weight_decay):\n    \"\"\"Create optimizer configuration dictionary.\"\"\"\n    return {\"lr\": lr, \"momentum\": momentum, \"weight_decay\": weight_decay}\n\n# Test (uncomment after adding hints):\n# print(calculate_loss([1.0, 2.0], [1.1, 2.2]))\n# print(get_batch([1,2,3,4,5], 0, 2))\n# print(create_optimizer_config(0.01, 0.9, 1e-4))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Docstrings\n",
    "\n",
    "Use Google-style docstrings for complex functions:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    epochs: int = 10,\n",
    "    learning_rate: float = 0.001,\n",
    "    verbose: bool = True\n",
    ") -> dict[str, list[float]]:\n",
    "    \"\"\"\n",
    "    Train a PyTorch model.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to train (nn.Module)\n",
    "        train_loader: DataLoader with training data\n",
    "        epochs: Number of training epochs\n",
    "        learning_rate: Learning rate for optimizer\n",
    "        verbose: Whether to print progress\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with 'train_loss' history\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If epochs < 1\n",
    "    \n",
    "    Example:\n",
    "        >>> history = train_model(model, loader, epochs=5)\n",
    "        >>> plt.plot(history['train_loss'])\n",
    "    \"\"\"\n",
    "    if epochs < 1:\n",
    "        raise ValueError(\"epochs must be >= 1\")\n",
    "    # ... training code ...\n",
    "    return {\"train_loss\": []}\n",
    "\n",
    "# For simple/obvious functions, a one-liner is fine:\n",
    "def relu(x: float) -> float:\n",
    "    \"\"\"Return max(0, x).\"\"\"\n",
    "    return max(0, x)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Pythonic Patterns\n",
    "\n",
    "### List Comprehensions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Instead of:\n",
    "squares_loop = []\n",
    "for i in range(10):\n",
    "    squares_loop.append(i ** 2)\n",
    "\n",
    "# Use:\n",
    "squares = [i ** 2 for i in range(10)]\n",
    "print(f\"Squares: {squares}\")\n",
    "\n",
    "# With condition\n",
    "evens = [i for i in range(20) if i % 2 == 0]\n",
    "print(f\"Evens: {evens}\")\n",
    "\n",
    "# Dict comprehension\n",
    "word_lengths = {word: len(word) for word in [\"cat\", \"elephant\", \"dog\"]}\n",
    "print(f\"Word lengths: {word_lengths}\")\n",
    "\n",
    "# Set comprehension (removes duplicates)\n",
    "unique_lengths = {len(word) for word in [\"cat\", \"bat\", \"elephant\", \"ant\"]}\n",
    "print(f\"Unique lengths: {unique_lengths}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### enumerate, zip, sorted"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# enumerate - get index and value\n",
    "fruits = [\"apple\", \"banana\", \"cherry\"]\n",
    "for i, fruit in enumerate(fruits):\n",
    "    print(f\"{i}: {fruit}\")\n",
    "\n",
    "# zip - iterate multiple sequences together\n",
    "names = [\"Alice\", \"Bob\", \"Charlie\"]\n",
    "scores = [85, 92, 78]\n",
    "for name, score in zip(names, scores):\n",
    "    print(f\"{name}: {score}\")\n",
    "\n",
    "# sorted with key function\n",
    "students = [(\"Alice\", 85), (\"Bob\", 92), (\"Charlie\", 78)]\n",
    "by_score = sorted(students, key=lambda x: x[1], reverse=True)\n",
    "print(f\"By score (desc): {by_score}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Q: When should you use a list comprehension vs a regular loop?</b></summary>\n",
    "\n",
    "**A:** Use comprehensions when:\n",
    "- Building a new list/dict/set from an iterable\n",
    "- The logic fits on one readable line\n",
    "\n",
    "Use regular loops when:\n",
    "- You need complex logic or multiple statements\n",
    "- You're modifying in place rather than creating new\n",
    "- Readability suffers from one-liner\n",
    "\n",
    "**Rule of thumb:** If you can't understand it in 5 seconds, use a loop.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Exercise: Pythonic Refactoring\n\nRefactor this verbose code to use Pythonic patterns:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# VERBOSE VERSION - refactor this to be Pythonic!\n\n# Task 1: Create list of (name, score) tuples where score > 80\nnames = [\"Alice\", \"Bob\", \"Charlie\", \"Diana\"]\nscores = [95, 72, 88, 65]\nhigh_scorers = []\nfor i in range(len(names)):\n    if scores[i] > 80:\n        high_scorers.append((names[i], scores[i]))\n\n# Task 2: Create dict mapping filename -> extension\nfiles = [\"data.csv\", \"model.pt\", \"config.json\", \"README.md\"]\nextensions = {}\nfor f in files:\n    parts = f.split(\".\")\n    name = parts[0]\n    ext = parts[1]\n    extensions[name] = ext\n\n# Task 3: Read file, count non-empty lines (use context manager!)\nf = open(\"test_file.txt\", \"w\")\nf.write(\"line1\\n\\nline2\\nline3\\n\")\nf.close()\n\nf = open(\"test_file.txt\", \"r\")\nlines = f.readlines()\nf.close()\ncount = 0\nfor line in lines:\n    if line.strip() != \"\":\n        count = count + 1\n\n# Cleanup\nimport os\nos.remove(\"test_file.txt\")\n\nprint(f\"High scorers: {high_scorers}\")\nprint(f\"Extensions: {extensions}\")\nprint(f\"Non-empty lines: {count}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<details>\n<summary><b>Solution: Pythonic Refactoring</b></summary>\n\n```python\n# Task 1: zip + list comprehension with filter\nhigh_scorers = [(n, s) for n, s in zip(names, scores) if s > 80]\n\n# Task 2: dict comprehension with split unpacking\nextensions = {f.split(\".\")[0]: f.split(\".\")[1] for f in files}\n# Or cleaner:\nextensions = {Path(f).stem: Path(f).suffix[1:] for f in files}\n\n# Task 3: context manager + sum with generator\nwith open(\"test_file.txt\", \"w\") as f:\n    f.write(\"line1\\n\\nline2\\nline3\\n\")\n\nwith open(\"test_file.txt\", \"r\") as f:\n    count = sum(1 for line in f if line.strip())\n```\n</details>",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Managers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Context managers ensure cleanup (files close, locks release, etc.)\n",
    "\n",
    "# File I/O - always use 'with'\n",
    "from pathlib import Path\n",
    "\n",
    "# Write\n",
    "with open(\"test.txt\", \"w\") as f:\n",
    "    f.write(\"Hello, World!\")\n",
    "\n",
    "# Read\n",
    "with open(\"test.txt\", \"r\") as f:\n",
    "    content = f.read()\n",
    "print(f\"File content: {content}\")\n",
    "\n",
    "# Clean up\n",
    "Path(\"test.txt\").unlink()\n",
    "\n",
    "# PyTorch example: disable gradients for inference\n",
    "import torch\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = x * 2  # No gradient tracking here\n",
    "print(f\"y.requires_grad: {y.requires_grad}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Basic try/except\ndef safe_divide(a: float, b: float) -> float | None:\n    try:\n        return a / b\n    except ZeroDivisionError:\n        print(\"Warning: Division by zero\")\n        return None\n\nprint(safe_divide(10, 2))\nprint(safe_divide(10, 0))\n\n# Multiple exception types with proper chaining\ndef parse_int(s: str) -> int:\n    try:\n        return int(s)\n    except ValueError as e:\n        raise ValueError(f\"Cannot parse '{s}' as integer\") from e  # Chain exceptions!\n    except TypeError as e:\n        raise TypeError(f\"Expected string, got {type(s)}\") from e\n\n# finally - always runs (cleanup)\ndef read_with_cleanup(filename: str) -> str:\n    f = None\n    try:\n        f = open(filename, \"r\")\n        return f.read()\n    except FileNotFoundError:\n        return \"\"\n    finally:\n        if f:\n            f.close()\n            print(\"File closed\")\n\n# Demo exception chaining\ntry:\n    parse_int(\"abc\")\nexcept ValueError as e:\n    print(f\"Caught: {e}\")\n    print(f\"Original cause: {e.__cause__}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Q: When should you catch exceptions vs let them propagate?</b></summary>\n",
    "\n",
    "**A:** \n",
    "- **Catch** when you can handle it meaningfully (retry, default value, cleanup)\n",
    "- **Propagate** when the caller should decide how to handle it\n",
    "\n",
    "**Bad:** Catching everything and hiding errors\n",
    "```python\n",
    "try:\n",
    "    result = do_something()\n",
    "except:  # Never do this!\n",
    "    pass\n",
    "```\n",
    "\n",
    "**Good:** Catch specific exceptions you can handle\n",
    "```python\n",
    "try:\n",
    "    data = load_file(path)\n",
    "except FileNotFoundError:\n",
    "    data = default_data\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: NumPy Essentials\n",
    "\n",
    "NumPy is the foundation for all deep learning frameworks. Understanding it is essential.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 Why NumPy Matters for Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Vectorization is MUCH faster than loops\n",
    "size = 1_000_000\n",
    "\n",
    "# Loop version\n",
    "a_list = list(range(size))\n",
    "b_list = list(range(size))\n",
    "\n",
    "start = time.time()\n",
    "c_list = [a + b for a, b in zip(a_list, b_list)]\n",
    "loop_time = time.time() - start\n",
    "\n",
    "# NumPy version\n",
    "a_np = np.arange(size)\n",
    "b_np = np.arange(size)\n",
    "\n",
    "start = time.time()\n",
    "c_np = a_np + b_np\n",
    "numpy_time = time.time() - start\n",
    "\n",
    "print(f\"Loop time: {loop_time:.4f}s\")\n",
    "print(f\"NumPy time: {numpy_time:.4f}s\")\n",
    "print(f\"NumPy is {loop_time/numpy_time:.1f}x faster\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "<details>\n<summary><b>Deep Dive: Why is NumPy so fast?</b></summary>\n\nNumPy achieves 10-100x speedups through several mechanisms:\n\n1. **Contiguous Memory Layout**: Arrays store data in continuous memory blocks, enabling efficient CPU cache utilization. Python lists store pointers to scattered objects.\n\n2. **Compiled C/Fortran Backend**: Core operations are implemented in optimized C code, not interpreted Python.\n\n3. **SIMD Vectorization**: Modern CPUs can process multiple numbers per instruction (Single Instruction, Multiple Data). NumPy operations leverage this automatically.\n\n4. **No Type Checking Per Element**: Python lists check types dynamically for each element. NumPy arrays have uniform dtype - no per-element overhead.\n\n5. **No Python Object Overhead**: Each Python object has ~28 bytes of overhead (reference count, type pointer, etc.). NumPy stores raw numbers.\n\n```python\n# Memory comparison\nimport sys\npy_list = [1.0] * 1000\nnp_array = np.ones(1000)\nprint(f\"Python list: {sys.getsizeof(py_list) + sum(sys.getsizeof(x) for x in py_list)} bytes\")\nprint(f\"NumPy array: {np_array.nbytes} bytes\")  # Just 8000 bytes (8 bytes per float64)\n```\n\n**Rule**: If you're looping over array elements in Python, you're probably doing it wrong.\n</details>",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Array Creation & Indexing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "# Creating arrays\n",
    "a = np.array([1, 2, 3, 4, 5])          # From list\n",
    "b = np.zeros((3, 4))                     # 3x4 zeros\n",
    "c = np.ones((2, 3))                      # 2x3 ones\n",
    "d = np.arange(0, 10, 2)                  # [0, 2, 4, 6, 8]\n",
    "e = np.linspace(0, 1, 5)                 # 5 points from 0 to 1\n",
    "f = np.random.randn(3, 3)                # 3x3 standard normal\n",
    "\n",
    "print(f\"zeros shape: {b.shape}\")\n",
    "print(f\"arange: {d}\")\n",
    "print(f\"linspace: {e}\")\n",
    "\n",
    "# Indexing\n",
    "arr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(f\"\\narr:\\n{arr}\")\n",
    "print(f\"arr[0, 1]: {arr[0, 1]}\")         # Single element\n",
    "print(f\"arr[0, :]: {arr[0, :]}\")         # First row\n",
    "print(f\"arr[:, 1]: {arr[:, 1]}\")         # Second column\n",
    "print(f\"arr[0:2, 1:3]:\\n{arr[0:2, 1:3]}\")  # Subarray\n",
    "\n",
    "# Boolean indexing\n",
    "print(f\"\\narr > 5: {arr[arr > 5]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3.3 Broadcasting\n\nBroadcasting allows operations between arrays of different shapes.\n\n### Rules:\n1. Compare shapes from right to left\n2. Dimensions match if they're equal OR one of them is 1\n3. Missing dimensions are treated as 1\n\n**Before running each cell below, predict the output shape!**"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "# Scalar broadcasts to any shape\n",
    "a = np.array([[1, 2, 3], [4, 5, 6]])  # Shape: (2, 3)\n",
    "print(f\"a + 10:\\n{a + 10}\")  # 10 broadcasts to (2, 3)\n",
    "\n",
    "# Row vector broadcasts across rows\n",
    "row = np.array([100, 200, 300])  # Shape: (3,)\n",
    "print(f\"\\na + row:\\n{a + row}\")  # (3,) -> (2, 3)\n",
    "\n",
    "# Column vector broadcasts across columns\n",
    "col = np.array([[10], [20]])  # Shape: (2, 1)\n",
    "print(f\"\\na + col:\\n{a + col}\")  # (2, 1) -> (2, 3)\n",
    "\n",
    "# Outer product via broadcasting\n",
    "x = np.array([1, 2, 3])[:, np.newaxis]  # Shape: (3, 1)\n",
    "y = np.array([10, 20])                   # Shape: (2,)\n",
    "print(f\"\\nOuter product (x * y):\\n{x * y}\")  # (3, 1) * (2,) -> (3, 2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Q: Why does `np.array([1,2]) + np.array([[1],[2],[3]])` work?</b></summary>\n",
    "\n",
    "**A:** Let's trace the broadcasting:\n",
    "- Left: shape (2,)\n",
    "- Right: shape (3, 1)\n",
    "\n",
    "Align from right:\n",
    "```\n",
    "     (2,)  ->  (1, 2)  [add dimension]\n",
    "  (3, 1)   ->  (3, 1)\n",
    "  Result:      (3, 2)  [both expand]\n",
    "```\n",
    "\n",
    "Each expands where it has size 1:\n",
    "```python\n",
    "[[1, 2],      [[1, 1],     [[2, 3],\n",
    " [1, 2],  +    [2, 2],  =   [3, 4],\n",
    " [1, 2]]       [3, 3]]      [4, 5]]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Broadcasting Debugger - useful helper function\ndef broadcast_shapes(*shapes):\n    \"\"\"Visualize how shapes align and what the result will be.\"\"\"\n    max_dims = max(len(s) for s in shapes)\n    \n    # Pad shapes with 1s on the left\n    padded = [((1,) * (max_dims - len(s))) + s for s in shapes]\n    \n    print(\"Shape alignment (right-aligned):\")\n    for i, (orig, pad) in enumerate(zip(shapes, padded)):\n        print(f\"  Array {i+1}: {str(orig):>15} -> {pad}\")\n    \n    # Compute result shape\n    result = []\n    for dims in zip(*padded):\n        if len(set(d for d in dims if d != 1)) > 1:\n            print(f\"\\n❌ INCOMPATIBLE: dimension has {dims} (multiple non-1 values)\")\n            return None\n        result.append(max(dims))\n    \n    print(f\"\\n✓ Result shape: {tuple(result)}\")\n    return tuple(result)\n\n# Test it\nprint(\"Example 1: (2,3) + (3,)\")\nbroadcast_shapes((2, 3), (3,))\n\nprint(\"\\nExample 2: (3,1) + (1,4)\")\nbroadcast_shapes((3, 1), (1, 4))\n\nprint(\"\\nExample 3: Incompatible shapes\")\nbroadcast_shapes((3, 4), (5,))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Broadcasting\n",
    "\n",
    "Fix the code to add bias to each sample:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data: 100 samples, 784 features (like MNIST flattened)\n",
    "X = np.random.randn(100, 784)\n",
    "bias = np.random.randn(784)\n",
    "\n",
    "# This should add bias to each row\n",
    "result = X + bias  # Does this work?\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"bias shape: {bias.shape}\")\n",
    "print(f\"result shape: {result.shape}\")\n",
    "assert result.shape == (100, 784), \"Shape mismatch!\"\n",
    "print(\"Broadcasting worked!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Common Operations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(f\"Original shape: {a.shape}\")\n",
    "\n",
    "# Reshape\n",
    "b = a.reshape(3, 2)\n",
    "print(f\"Reshaped to (3,2):\\n{b}\")\n",
    "\n",
    "# Transpose\n",
    "print(f\"Transposed:\\n{a.T}\")\n",
    "\n",
    "# Flatten\n",
    "print(f\"Flattened: {a.flatten()}\")\n",
    "\n",
    "# Concatenate\n",
    "c = np.array([[7, 8, 9]])\n",
    "print(f\"\\nVertical concat:\\n{np.concatenate([a, c], axis=0)}\")\n",
    "\n",
    "d = np.array([[10], [20]])\n",
    "print(f\"\\nHorizontal concat:\\n{np.concatenate([a, d], axis=1)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Reductions along axes\n",
    "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(f\"Array:\\n{a}\")\n",
    "\n",
    "print(f\"\\nSum all: {a.sum()}\")\n",
    "print(f\"Sum rows (axis=1): {a.sum(axis=1)}\")     # Sum each row\n",
    "print(f\"Sum cols (axis=0): {a.sum(axis=0)}\")     # Sum each column\n",
    "\n",
    "print(f\"\\nMean all: {a.mean():.2f}\")\n",
    "print(f\"Mean rows: {a.mean(axis=1)}\")\n",
    "\n",
    "# Matrix multiplication\n",
    "W = np.random.randn(3, 4)  # 3x4\n",
    "x = np.random.randn(4, 2)  # 4x2\n",
    "y = W @ x                   # 3x2\n",
    "print(f\"\\nW @ x: {W.shape} @ {x.shape} = {y.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Q: What's the difference between `axis=0` and `axis=1` in reductions?</b></summary>\n",
    "\n",
    "**A:** The axis parameter specifies which dimension to \"collapse\":\n",
    "- `axis=0`: Collapse rows → result has shape of a single row\n",
    "- `axis=1`: Collapse columns → result has shape of a single column\n",
    "\n",
    "Think of it as: \"sum **along** this axis\" or \"reduce **this** dimension\"\n",
    "\n",
    "```python\n",
    "a = [[1, 2, 3],\n",
    "     [4, 5, 6]]  # Shape (2, 3)\n",
    "\n",
    "a.sum(axis=0)  # [5, 7, 9]   - summed down columns, shape (3,)\n",
    "a.sum(axis=1)  # [6, 15]     - summed across rows, shape (2,)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Exercise: Shape Prediction\n\n**Predict the output shapes before running!** Write your predictions, then verify.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\n\n# VIEWS: Slicing creates a view (shares memory!)\noriginal = np.array([1, 2, 3, 4, 5])\nview = original[1:4]  # This is a VIEW\n\nprint(f\"Original: {original}\")\nprint(f\"View: {view}\")\n\n# Modifying the view changes the original!\nview[0] = 999\nprint(f\"After modifying view[0]:\")\nprint(f\"  Original: {original}\")  # Also changed!\nprint(f\"  View: {view}\")\n\n# COPIES: Use .copy() to get independent data\noriginal = np.array([1, 2, 3, 4, 5])\ncopy = original[1:4].copy()  # Explicit copy\n\ncopy[0] = 999\nprint(f\"\\nWith .copy():\")\nprint(f\"  Original: {original}\")  # Unchanged!\nprint(f\"  Copy: {copy}\")\n\n# How to check: views share memory\na = np.array([1, 2, 3])\nb = a[:]      # View\nc = a.copy()  # Copy\n\nprint(f\"\\nShares memory?\")\nprint(f\"  a and b: {np.shares_memory(a, b)}\")  # True\nprint(f\"  a and c: {np.shares_memory(a, c)}\")  # False",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<details>\n<summary><b>Q: Is `arr.reshape(3, 4)` a view or a copy?</b></summary>\n\n**A:** It depends! Reshape returns a **view** when possible (if the data is contiguous in memory), but may return a **copy** if the memory layout doesn't allow a view.\n\n```python\na = np.arange(12).reshape(3, 4)  # Usually a view\nb = a.T.reshape(6, 2)            # Must be a copy (transpose breaks contiguity)\n```\n\n**Safe approach:** If you need to be sure, use `.copy()` explicitly. If you want to ensure a view (and error otherwise), use `.reshape()` with `order='A'` or `np.ndarray.view()`.\n</details>",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 3.5 Views vs Copies (Critical!)\n\nUnderstanding when NumPy creates a view vs a copy prevents subtle bugs.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: OOP for Deep Learning\n",
    "\n",
    "PyTorch heavily uses OOP. Understanding these patterns is essential.\n",
    "\n",
    "---\n",
    "\n",
    "## 4.1 Classes Review"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"A simple neural network class demonstrating OOP patterns.\"\"\"\n",
    "    \n",
    "    # Class attribute (shared by all instances)\n",
    "    default_activation = \"relu\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        \"\"\"Initialize the network.\"\"\"\n",
    "        # Instance attributes (unique to each instance)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Simulated weights\n",
    "        self.weights = {\n",
    "            \"W1\": np.random.randn(input_size, hidden_size) * 0.01,\n",
    "            \"W2\": np.random.randn(hidden_size, output_size) * 0.01,\n",
    "        }\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        h = x @ self.weights[\"W1\"]\n",
    "        h = np.maximum(0, h)  # ReLU\n",
    "        return h @ self.weights[\"W2\"]\n",
    "\n",
    "# Usage\n",
    "net = NeuralNetwork(784, 128, 10)\n",
    "x = np.random.randn(32, 784)  # Batch of 32\n",
    "output = net.forward(x)\n",
    "print(f\"Input: {x.shape} -> Output: {output.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Naming Conventions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"Demonstrates Python naming conventions.\"\"\"\n",
    "    \n",
    "    def __init__(self, data: list):\n",
    "        self.data = data              # Public: anyone can access\n",
    "        self._cache = {}              # Protected: internal use, but accessible\n",
    "        self.__secret = \"hidden\"      # Private: name-mangled to _DataProcessor__secret\n",
    "    \n",
    "    def process(self):\n",
    "        \"\"\"Public method - part of the API.\"\"\"\n",
    "        return self._preprocess()\n",
    "    \n",
    "    def _preprocess(self):\n",
    "        \"\"\"Protected method - internal helper, but subclasses can override.\"\"\"\n",
    "        return [x * 2 for x in self.data]\n",
    "    \n",
    "    def __validate(self):\n",
    "        \"\"\"Private method - truly internal, not for subclasses.\"\"\"\n",
    "        return all(isinstance(x, (int, float)) for x in self.data)\n",
    "\n",
    "dp = DataProcessor([1, 2, 3])\n",
    "print(f\"Public data: {dp.data}\")\n",
    "print(f\"Protected _cache: {dp._cache}\")  # Works but discouraged\n",
    "# print(dp.__secret)  # AttributeError!\n",
    "print(f\"Mangled name: {dp._DataProcessor__secret}\")  # How to access if needed"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Q: When should you use `_protected` vs `__private`?</b></summary>\n",
    "\n",
    "**A:**\n",
    "- **`_protected`**: Use for internal methods that subclasses might need to override. It's a convention saying \"internal, but accessible.\"\n",
    "\n",
    "- **`__private`**: Use when you truly want to prevent accidental override in subclasses. Python mangles the name to `_ClassName__method`, making it harder (but not impossible) to access.\n",
    "\n",
    "**In practice:** Most Python code uses `_protected`. Use `__private` sparingly.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Dunder Methods\n",
    "\n",
    "Dunder (double underscore) methods let you customize how objects behave."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class Tensor:\n",
    "    \"\"\"A simple tensor class demonstrating dunder methods.\"\"\"\n",
    "    \n",
    "    def __init__(self, data: list):\n",
    "        self.data = np.array(data)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"For developers - unambiguous representation.\"\"\"\n",
    "        return f\"Tensor(shape={self.data.shape}, dtype={self.data.dtype})\"\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"For users - readable representation.\"\"\"\n",
    "        return f\"Tensor with shape {self.data.shape}\"\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Enable len(tensor).\"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Enable tensor[idx].\"\"\"\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Enable tensor(x) - used heavily in PyTorch!\"\"\"\n",
    "        return self.data @ x\n",
    "\n",
    "t = Tensor([[1, 2], [3, 4]])\n",
    "print(f\"repr: {repr(t)}\")\n",
    "print(f\"str: {str(t)}\")\n",
    "print(f\"len: {len(t)}\")\n",
    "print(f\"t[0]: {t[0]}\")\n",
    "print(f\"t([1, 1]): {t(np.array([1, 1]))}\")  # Callable!"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Q: Why does PyTorch use `__call__` for the forward pass?</b></summary>\n",
    "\n",
    "**A:** In PyTorch, `model(x)` calls `model.__call__(x)`, which internally calls `model.forward(x)` but also handles:\n",
    "- Hooks (callbacks before/after forward)\n",
    "- Gradient tracking setup\n",
    "- Module state management\n",
    "\n",
    "This is why you define `forward()` but call `model(x)`, not `model.forward(x)`.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 4.5 Inheritance (Essential for PyTorch)\n\nPyTorch's `nn.Module` uses inheritance heavily. You'll subclass it for every model.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "<details>\n<summary><b>Q: Why do we call `super().__init__()` in subclasses?</b></summary>\n\n**A:** `super().__init__()` calls the parent class's `__init__` method, ensuring proper initialization of inherited attributes. Without it:\n\n```python\nclass Linear(Module):\n    def __init__(self, in_features, out_features):\n        # WRONG: forgot super().__init__()\n        self.weight = ...\n        \nlayer = Linear(10, 5)\nprint(layer.training)  # AttributeError! .training was never set\n```\n\nIn PyTorch, forgetting `super().__init__()` is a common bug that breaks module registration, parameter tracking, and device movement.\n</details>",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Simplified nn.Module-like base class\nclass Module:\n    \"\"\"Base class demonstrating PyTorch's Module pattern.\"\"\"\n    \n    def __init__(self):\n        self._modules = {}\n        self.training = True\n    \n    def __call__(self, x):\n        \"\"\"When you call model(x), this runs.\"\"\"\n        return self.forward(x)\n    \n    def forward(self, x):\n        \"\"\"Subclasses MUST override this.\"\"\"\n        raise NotImplementedError(\"Subclasses must implement forward()\")\n    \n    def train(self, mode: bool = True):\n        self.training = mode\n        return self\n    \n    def eval(self):\n        return self.train(False)\n\n\n# Subclass: A simple linear layer\nclass Linear(Module):\n    \"\"\"Linear layer: y = x @ W + b\"\"\"\n    \n    def __init__(self, in_features: int, out_features: int):\n        super().__init__()  # Call parent's __init__\n        self.weight = np.random.randn(in_features, out_features) * 0.01\n        self.bias = np.zeros(out_features)\n    \n    def forward(self, x: np.ndarray) -> np.ndarray:\n        return x @ self.weight + self.bias  # Broadcasting!\n\n\n# Subclass: A two-layer network\nclass TwoLayerNet(Module):\n    \"\"\"Network that composes multiple layers.\"\"\"\n    \n    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n        super().__init__()\n        self.fc1 = Linear(input_size, hidden_size)\n        self.fc2 = Linear(hidden_size, output_size)\n    \n    def forward(self, x: np.ndarray) -> np.ndarray:\n        x = self.fc1(x)           # Note: uses __call__, not .forward()\n        x = np.maximum(0, x)      # ReLU activation\n        x = self.fc2(x)\n        return x\n\n\n# Usage - this is exactly how you'll use PyTorch!\nmodel = TwoLayerNet(784, 128, 10)\nx = np.random.randn(32, 784)\noutput = model(x)  # Calls __call__ -> forward\nprint(f\"Input: {x.shape} -> Output: {output.shape}\")\nprint(f\"Training mode: {model.training}\")\nmodel.eval()\nprint(f\"After eval(): {model.training}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Decorators"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class Model:\n",
    "    def __init__(self, name: str):\n",
    "        self._name = name\n",
    "        self._is_training = True\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        \"\"\"Property decorator - access like an attribute.\"\"\"\n",
    "        return self._name\n",
    "    \n",
    "    @property\n",
    "    def is_training(self) -> bool:\n",
    "        return self._is_training\n",
    "    \n",
    "    @is_training.setter\n",
    "    def is_training(self, value: bool):\n",
    "        \"\"\"Setter for property.\"\"\"\n",
    "        self._is_training = value\n",
    "        print(f\"Training mode: {value}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def count_parameters(weights: dict) -> int:\n",
    "        \"\"\"Static method - doesn't need self.\"\"\"\n",
    "        return sum(w.size for w in weights.values())\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config: dict):\n",
    "        \"\"\"Class method - alternative constructor.\"\"\"\n",
    "        return cls(name=config.get(\"name\", \"unnamed\"))\n",
    "\n",
    "# Usage\n",
    "m = Model(\"MyModel\")\n",
    "print(f\"Name: {m.name}\")  # Property access\n",
    "m.is_training = False     # Property setter\n",
    "\n",
    "m2 = Model.from_config({\"name\": \"ConfigModel\"})  # Classmethod\n",
    "print(f\"From config: {m2.name}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Practical Patterns\n",
    "\n",
    "---\n",
    "\n",
    "## 5.1 Generators & Iterators\n",
    "\n",
    "Generators are crucial for memory-efficient data loading."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generator function - uses yield\n",
    "def count_up_to(n: int):\n",
    "    \"\"\"Generate numbers from 0 to n-1.\"\"\"\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        yield i  # Pauses here, returns value\n",
    "        i += 1\n",
    "\n",
    "# Usage\n",
    "for num in count_up_to(5):\n",
    "    print(num, end=\" \")\n",
    "print()\n",
    "\n",
    "# Generator expression (like list comprehension but lazy)\n",
    "squares_gen = (x**2 for x in range(1000000))  # No memory allocated yet!\n",
    "print(f\"Generator: {squares_gen}\")\n",
    "print(f\"First 5: {[next(squares_gen) for _ in range(5)]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Why generators matter for DL: memory efficiency\n",
    "import sys\n",
    "\n",
    "# List stores all values in memory\n",
    "big_list = [i**2 for i in range(1000000)]\n",
    "print(f\"List size: {sys.getsizeof(big_list) / 1e6:.1f} MB\")\n",
    "\n",
    "# Generator computes on-demand\n",
    "def big_gen():\n",
    "    for i in range(1000000):\n",
    "        yield i**2\n",
    "\n",
    "gen = big_gen()\n",
    "print(f\"Generator size: {sys.getsizeof(gen)} bytes\")\n",
    "\n",
    "# DataLoader-style batching\n",
    "def batch_generator(data: list, batch_size: int):\n",
    "    \"\"\"Yield batches from data.\"\"\"\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i + batch_size]\n",
    "\n",
    "data = list(range(100))\n",
    "for batch in batch_generator(data, batch_size=32):\n",
    "    print(f\"Batch: {batch[:3]}... (size {len(batch)})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Q: When should you use a generator vs a list?</b></summary>\n",
    "\n",
    "**A:**\n",
    "- **Generator**: When data is large, you only need one pass, or values are computed on-demand\n",
    "- **List**: When you need random access, multiple passes, or the data is small\n",
    "\n",
    "**DataLoaders use generators** because:\n",
    "1. Training data is often huge (can't fit in RAM)\n",
    "2. You only need one batch at a time\n",
    "3. Data can be augmented on-the-fly\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 File I/O with Pathlib"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create paths (cross-platform!)\n",
    "data_dir = Path(\"data\")\n",
    "model_path = data_dir / \"models\" / \"best.pt\"\n",
    "\n",
    "print(f\"Path: {model_path}\")\n",
    "print(f\"Parent: {model_path.parent}\")\n",
    "print(f\"Name: {model_path.name}\")\n",
    "print(f\"Stem: {model_path.stem}\")\n",
    "print(f\"Suffix: {model_path.suffix}\")\n",
    "\n",
    "# Check existence\n",
    "print(f\"\\nExists: {model_path.exists()}\")\n",
    "print(f\"Is file: {model_path.is_file()}\")\n",
    "\n",
    "# Find files\n",
    "current = Path(\".\")\n",
    "print(f\"\\nPython files in current dir: {list(current.glob('*.py'))[:3]}\")\n",
    "print(f\"All .ipynb (recursive): {list(current.glob('**/*.ipynb'))[:3]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Debugging Strategies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 1. Print debugging with f-strings\n",
    "def debug_forward(x, W):\n",
    "    print(f\"DEBUG: x.shape={x.shape}, W.shape={W.shape}\")\n",
    "    result = x @ W\n",
    "    print(f\"DEBUG: result.shape={result.shape}\")\n",
    "    return result\n",
    "\n",
    "# 2. Assertions - catch bugs early\n",
    "def normalize(x: np.ndarray) -> np.ndarray:\n",
    "    assert x.ndim == 2, f\"Expected 2D array, got {x.ndim}D\"\n",
    "    assert x.shape[0] > 0, \"Empty array\"\n",
    "    return (x - x.mean(axis=0)) / (x.std(axis=0) + 1e-8)\n",
    "\n",
    "# 3. Shape annotations in comments\n",
    "def attention(Q, K, V):\n",
    "    # Q: (batch, heads, seq_len, d_k)\n",
    "    # K: (batch, heads, seq_len, d_k)\n",
    "    # V: (batch, heads, seq_len, d_v)\n",
    "    \n",
    "    scores = Q @ K.transpose(-2, -1)  # (batch, heads, seq_len, seq_len)\n",
    "    weights = scores  # Simplified - normally softmax\n",
    "    output = weights @ V  # (batch, heads, seq_len, d_v)\n",
    "    return output\n",
    "\n",
    "# Test\n",
    "x = np.random.randn(32, 784)\n",
    "W = np.random.randn(784, 128)\n",
    "y = debug_forward(x, W)\n",
    "z = normalize(x)\n",
    "print(f\"\\nNormalized shape: {z.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Summary & Exercises\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### Python Foundations\n",
    "- Use **type hints** (`def f(x: int) -> str`) for self-documenting code\n",
    "- Use **comprehensions** for building collections, loops for complex logic\n",
    "- Use **context managers** (`with`) for resource management\n",
    "- **Catch specific exceptions**, let others propagate\n",
    "\n",
    "### NumPy\n",
    "- **Vectorize** operations - avoid Python loops on arrays\n",
    "- **Broadcasting** aligns shapes from the right, expanding size-1 dimensions\n",
    "- **axis=0** collapses rows, **axis=1** collapses columns\n",
    "\n",
    "### OOP for DL\n",
    "- **`_protected`** for internal methods, **`__private`** rarely\n",
    "- **`__call__`** makes objects callable (used by `nn.Module`)\n",
    "- **`@property`** for computed attributes, **`@classmethod`** for alternative constructors\n",
    "\n",
    "### Practical Patterns\n",
    "- **Generators** for memory-efficient iteration (DataLoaders!)\n",
    "- **Pathlib** for cross-platform file paths\n",
    "- **Shape comments** for debugging tensor operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Assessment Checklist\n",
    "\n",
    "Before proceeding to Lab 6, you should be able to:\n",
    "\n",
    "- [ ] Write a function with type hints and a Google-style docstring\n",
    "- [ ] Convert a loop to a list comprehension\n",
    "- [ ] Explain what `axis=0` vs `axis=1` means in NumPy reductions\n",
    "- [ ] Predict the output shape of broadcasting `(3,1) + (4,)`\n",
    "- [ ] Explain why `__call__` is used in PyTorch modules\n",
    "- [ ] Write a generator function with `yield`\n",
    "- [ ] Use `pathlib.Path` to construct file paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Exercise: Mini Data Pipeline\n",
    "\n",
    "Implement a simple data pipeline using the concepts from this lab:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Exercise: Implement a data pipeline class\n",
    "\n",
    "class DataPipeline:\n",
    "    \"\"\"\n",
    "    A simple data pipeline for loading and batching data.\n",
    "    \n",
    "    TODO: Implement the following methods:\n",
    "    1. __init__: Store data and batch_size\n",
    "    2. __len__: Return number of batches\n",
    "    3. __iter__: Yield batches (use a generator!)\n",
    "    4. normalize: Normalize data using broadcasting\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data: np.ndarray, batch_size: int = 32):\n",
    "        # TODO: Store data and batch_size\n",
    "        pass\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        # TODO: Return number of batches (hint: use ceiling division)\n",
    "        pass\n",
    "    \n",
    "    def __iter__(self):\n",
    "        # TODO: Yield batches of data\n",
    "        pass\n",
    "    \n",
    "    def normalize(self) -> np.ndarray:\n",
    "        # TODO: Return normalized data (zero mean, unit std per feature)\n",
    "        pass\n",
    "\n",
    "# Test your implementation:\n",
    "# data = np.random.randn(100, 784)\n",
    "# pipeline = DataPipeline(data, batch_size=32)\n",
    "# print(f\"Number of batches: {len(pipeline)}\")\n",
    "# for i, batch in enumerate(pipeline):\n",
    "#     print(f\"Batch {i}: shape {batch.shape}\")\n",
    "# normalized = pipeline.normalize()\n",
    "# print(f\"Mean after normalization: {normalized.mean(axis=0)[:5]}\")  # Should be ~0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "class DataPipeline:\n",
    "    def __init__(self, data: np.ndarray, batch_size: int = 32):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return (len(self.data) + self.batch_size - 1) // self.batch_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.data), self.batch_size):\n",
    "            yield self.data[i:i + self.batch_size]\n",
    "    \n",
    "    def normalize(self) -> np.ndarray:\n",
    "        mean = self.data.mean(axis=0)  # Shape: (784,)\n",
    "        std = self.data.std(axis=0)    # Shape: (784,)\n",
    "        return (self.data - mean) / (std + 1e-8)  # Broadcasting!\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. [Python Type Hints Cheat Sheet](https://mypy.readthedocs.io/en/stable/cheat_sheet_py3.html)\n",
    "2. [NumPy Broadcasting Rules](https://numpy.org/doc/stable/user/basics.broadcasting.html)\n",
    "3. [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html)\n",
    "4. [Real Python - Generators](https://realpython.com/introduction-to-python-generators/)\n",
    "5. [PyTorch nn.Module Source](https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/module.py)\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** Lab 6 - RNN Foundations"
   ]
  }
 ]
}